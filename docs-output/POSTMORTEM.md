# Sprocket Infrastructure Project Postmortem

**Project**: Sprocket Platform Infrastructure v1.0
**Timeline**: September 14, 2025 - November 8, 2025 (8 weeks)
**Status**: ‚úÖ Successfully Deployed to Production
**Team**: Infrastructure Engineering
**Author**: Generated from project documentation and git history

---

## Executive Summary

This postmortem documents the complete journey of rebuilding the Sprocket gaming platform infrastructure from a partially-functional, outdated deployment to a fully production-ready system serving real users. The project took 8 weeks and involved fundamental architectural changes, migration to managed services, and resolution of complex technical challenges.

**Key Outcomes**:
- ‚úÖ Production deployment complete and serving users
- ‚úÖ Three-layer architecture (Infrastructure ‚Üí Data ‚Üí Applications)
- ‚úÖ Migrated to managed PostgreSQL (Digital Ocean)
- ‚úÖ Migrated to cloud S3 storage (AWS S3 + Digital Ocean Spaces)
- ‚úÖ Automated Vault unsealing and secret provisioning
- ‚úÖ Multi-environment routing support (local, LAN, cloud)
- ‚úÖ HTTPS with Let's Encrypt automation
- ‚úÖ Comprehensive monitoring and observability

**Final Infrastructure**:
- **Layer 1**: Traefik, Vault, Socket Proxy
- **Layer 2**: Redis, RabbitMQ, InfluxDB, Grafana, N8n, Neo4j, Gatus, Loki, Telegraf
- **Platform**: Web UI, API, Discord Bot, Image Generation, 7 microservices

---

## Project Timeline

### Visual Timeline Overview

```
September 2025                        October 2025                     November 2025
|                                     |                                |
Week 1     Week 2     Week 3    Week 4-5      Week 6-7         Week 8
|          |          |          |             |                |
Sept 14    Sept 18    Sept 30   Oct 26        Oct 26-27        Nov 4-8
|          |          |          |             |                |
v          v          v          v             v                v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Phase 1 ‚îÇ‚îÇ Phase 2 ‚îÇ‚îÇ Phase 3 ‚îÇ‚îÇ Phase 4 ‚îÇ ‚îÇ Phase 5 ‚îÇ     ‚îÇ Phase 6 ‚îÇ
‚îÇFoundation‚îÇ Vault   ‚îÇ‚îÇ Storage ‚îÇ‚îÇPlatform ‚îÇ ‚îÇ Routing ‚îÇ     ‚îÇ  Final  ‚îÇ
‚îÇ Rebuild ‚îÇ‚îÇStruggles‚îÇ‚îÇMigration‚îÇ‚îÇResurrect‚îÇ ‚îÇ  Hell   ‚îÇ     ‚îÇ  Push   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚úì          ‚úì          ‚úì          ‚úì           ‚úì               ‚úì
 Layer 1    Vault      MinIO‚ÜíS3   Platform    Multi-env      Production
 Running    Unseals    Complete   Services    Routing        Complete!
                                  Running     Working

Key Milestones:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Sept 14  ‚îÇ Initial deployment - Layer 1 running
Sept 19  ‚îÇ üéâ Vault unsealing breakthrough!
Sept 21  ‚îÇ Vault fully automated
Sept 30  ‚îÇ S3 migration begins
Oct 8    ‚îÇ MinIO completely removed
Oct 26   ‚îÇ üéâ "Sprocket is alive!" - Platform deployed
Oct 27   ‚îÇ Multi-environment routing solved
Nov 1-3  ‚îÇ Database migrated to managed PostgreSQL
Nov 7    ‚îÇ Platform running on production domain
Nov 8    ‚îÇ üéâüéâüéâ PRODUCTION COMPLETE - v1.0 live!
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Total Duration: 8 weeks (56 days)
Major Breakthroughs: 5
Critical Blockers Resolved: 3
Services Deployed: 22
```

### Effort Breakdown by Phase

| Phase | Duration | Complexity | Key Challenge | Outcome |
|-------|----------|------------|---------------|---------|
| **Phase 1: Foundation** | 5 days | ‚≠ê‚≠ê | Getting rusty infrastructure working | ‚úÖ Layer 1 operational |
| **Phase 2: Vault** | 3 days | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Automating Vault unsealing | ‚úÖ Fully automated |
| **Phase 3: Storage** | 8 days | ‚≠ê‚≠ê‚≠ê | Migrating off MinIO | ‚úÖ Cloud S3 operational |
| **Phase 4: Platform** | 1 day | ‚≠ê‚≠ê | Service dependencies | ‚úÖ All services running |
| **Phase 5: Routing** | 2 days | ‚≠ê‚≠ê‚≠ê‚≠ê | Multi-environment access | ‚úÖ All access patterns work |
| **Phase 6: Final Push** | 5 days | ‚≠ê‚≠ê‚≠ê | Production readiness | ‚úÖ Production live! |

**Most Challenging Phase**: Phase 2 (Vault Struggles) - 5 stars
**Quickest Win**: Phase 4 (Platform Resurrection) - 1 day after months of prep
**Longest Phase**: Phase 3 (Storage Migration) - 8 days for safe, phased migration

---

### Phase 1: Foundation Rebuild (Sept 14-19, 2025)
**Goal**: Get Layer 1 infrastructure working again

#### Key Milestones
- **Sept 14**: Initial deployment - Layer 1 running
- **Sept 16**: Layer 2 configuration started
- **Sept 17-18**: Vault bootstrap challenges begin

#### Challenges
- Infrastructure hadn't been deployed in months (since 2024)
- Vault initialization process broken
- MinIO + Vault integration issues
- Secret bootstrapping not automated

#### Outcomes
- Layer 1 deployable (Traefik + Vault + Socket Proxy)
- Basic networking established
- Identified critical blocker: Vault unsealing

**Commits**: `3126c6d`, `8e2c54d`, `5de5d04`

---

### Phase 2: The Vault Struggles (Sept 18-21, 2025)
**Goal**: Automate Vault initialization and unsealing

This phase was the most challenging part of the project. Vault's security model requires manual unsealing after each start, creating a chicken-and-egg problem for automation.

#### The Problem
```
Services need secrets from Vault
    ‚Üì
Vault must be unsealed to serve secrets
    ‚Üì
Unsealing requires unseal keys
    ‚Üì
Unseal keys must be stored securely
    ‚Üì
Where do we store them safely?
```

#### Attempts and Failures
1. **Attempt 1**: Store unseal keys in Pulumi config
   - **Result**: Security concern - even encrypted, this felt wrong

2. **Attempt 2**: Manual unsealing after each deployment
   - **Result**: Not repeatable, requires human intervention

3. **Attempt 3**: Store keys in MinIO
   - **Result**: MinIO needs Vault for credentials (circular dependency)

#### Breakthrough (Sept 19)
- **Commit `5057afc`**: "vault actually unseals!"
- Solution: Store unseal keys in local bind mount
  - Tokens stored at `global/services/vault/unseal-tokens/`
  - Auto-initialization script checks for existing tokens
  - Script unseals Vault automatically on container start
  - S3 backend ensures Vault state persists

#### Implementation
```bash
# auto-initialize.sh
1. Check if Vault is already initialized (via S3 backend)
2. If not initialized:
   - Run `vault operator init`
   - Save tokens to bind-mounted directory
3. If initialized but sealed:
   - Read unseal keys from bind-mounted directory
   - Unseal with 3 of 5 keys
4. Vault ready to serve secrets
```

#### Final Success (Sept 21)
- **Commit `2103358`**: "all tokens working now. Vault should be good!"
- Vault reliably initializes and unseals
- Services can retrieve secrets
- Repeatable deployment achieved

**Lessons Learned**:
- Security vs. automation trade-offs are real
- Local bind mounts are acceptable for node-specific secrets
- S3 backend is crucial for Vault state persistence
- Test the full initialization‚Üíunsealing‚Üísecret retrieval flow

---

### Phase 3: Storage Migration (Sept 30 - Oct 8, 2025)
**Goal**: Move from self-hosted MinIO to cloud S3 storage

#### Why Migrate?
**Problems with MinIO**:
- Required significant resources (CPU, memory, storage)
- Another service to maintain and monitor
- Vault S3 backend had instability with self-hosted MinIO
- Backup complexity (backing up the backup system)
- Single point of failure

**Benefits of Cloud S3**:
- Managed service (no maintenance)
- Built-in replication and durability (99.999999999%)
- Professional-grade reliability
- Lower operational overhead
- Better performance

#### Migration Process

**Phase 3a: Dual Operation (Sept 30)**
- **Commit `3f32a74`**: "migrate to cloud S3 and fix service configurations"
- Kept MinIO running
- Migrated Vault backend to Digital Ocean Spaces
- Updated `SprocketMinioProvider` to support direct credentials
- Tested stability for several days

**Phase 3b: Service Migration (Oct 1-7)**
- Updated application services to use cloud S3
- Migrated existing data (images, replays, backups)
- Verified all services working with new storage

**Phase 3c: Cleanup (Oct 8)**
- **Commit `3b45f27`**: "Move from MinIO to AWS. Remove postgres network."
- Removed MinIO service completely
- Cleaned up MinIO-related configuration
- Updated documentation

#### Configuration Changes
```yaml
# Before (MinIO)
s3-endpoint: minio.sprocket.mlesports.gg
s3-access-key: local-minio-key
s3-secret-key: local-minio-secret
s3-port: 9000
ssl: false

# After (Cloud S3)
s3-endpoint: nyc3.digitaloceanspaces.com
s3-access-key: DO004ZHPV38R8C9Q46XG
s3-secret-key: [from Vault]
s3-port: 443
ssl: true
```

#### Results
- ‚úÖ Vault backend: Digital Ocean Spaces (vault-secrets bucket)
- ‚úÖ Application storage: AWS S3 / Digital Ocean Spaces
- ‚úÖ Reduced infrastructure complexity
- ‚úÖ Better reliability and durability
- ‚úÖ One less service to manage

**Lessons Learned**:
- Managed services reduce operational burden significantly
- S3-compatible APIs make migration relatively easy
- Test storage backends thoroughly before full migration
- Migrate in phases to reduce risk

---

### Phase 4: Platform Resurrection (Oct 26, 2025)
**Goal**: Get application services running

After fixing infrastructure and storage, it was time to deploy the actual platform.

#### Challenges
- Services hadn't been deployed in months
- Configuration drift between local and production
- Dependency management (services depend on Layer 2)
- Database schema migrations needed
- Image versions and tags mismatched

#### Breakthrough
- **Commit `6395d01`**: "Sprocket is alive!"
- All platform services successfully deployed
- Web UI accessible
- API responding
- Discord bot connecting
- Microservices communicating

#### What Made It Work
1. **Proper layer ordering**: Layer 1 ‚Üí Layer 2 ‚Üí Platform
2. **Secret provisioning**: Ran `bootstrap-vault-secrets.sh` before platform deployment
3. **Database ready**: Managed PostgreSQL already provisioned
4. **Storage ready**: S3 buckets created and accessible
5. **Network configuration**: All services on correct networks

---

### Phase 5: Routing Hell (Oct 26-27, 2025)
**Goal**: Support multiple access patterns

#### The Problem
The platform needed to be accessible via:
1. **Local development**: `.localhost` domains (127.0.0.1)
2. **LAN access**: Direct IP (192.168.4.39)
3. **Tailscale access**: Tailscale IP (100.110.185.84)
4. **Production**: Real domain (sprocket.mlesports.gg)

Each pattern requires different Traefik routing rules and certificate handling.

#### Symptoms
- ‚úó Accessing via `http://localhost` ‚Üí 404 error
- ‚úó Accessing via `http://192.168.4.39` ‚Üí 404 error
- ‚úó Accessing via domain ‚Üí works but only from internet
- ‚úó Certificate errors everywhere

#### Root Cause
Traefik routes based on `Host` header:
```
Request to http://localhost
  ‚Üí Host: localhost
  ‚Üí Traefik rule: Host(`sprocket.mlesports.gg`)
  ‚Üí No match ‚Üí 404
```

#### Solution (Oct 27)
- **Commit `7486e02`**: "add localhost-based routing and cloud deployment documentation"

**Multi-pattern routing**:
```typescript
// Platform.ts
const hostRules = [];

// Hostname-based routing (production)
if (hostname) {
  hostRules.push(`Host(\`${subdomain}.${hostname}\`)`);
}

// IP-based routing (LAN access)
if (serverIp) {
  hostRules.push(`Host(\`${serverIp}\`)`);
}

// Tailscale routing
if (tailscaleIp) {
  hostRules.push(`Host(\`${tailscaleIp}\`)`);
}

const routingRule = hostRules.join(' || ');
```

**Result**:
```
Host(`sprocket.mlesports.gg`) || Host(`192.168.4.39`) || Host(`100.110.185.84`)
```

#### Configuration Strategy
**For Local Development**:
```yaml
hostname: localhost
server-ip: 192.168.4.39
tailscale-ip: 100.110.185.84
```

**For Production**:
```yaml
hostname: sprocket.mlesports.gg
# Remove server-ip and tailscale-ip
```

#### Additional Changes
- Created `CLOUD_DEPLOYMENT.md` - production deployment guide
- Created `LOCAL_ACCESS.md` - local development guide
- Fixed duplicate router issues causing 404s
- Documented `/etc/hosts` workaround for local development

**Lessons Learned**:
- Design for production first, add development overrides later
- IP-based routing is a hack, use proper DNS when possible
- Document different access patterns clearly
- Test each access method independently
- Traefik routing rules can get complex fast

---

### Phase 6: Database Migration (Nov 1-3, 2025)
**Goal**: Move to managed PostgreSQL

#### The Decision
Although PostgreSQL was working in Layer 2, the team decided to migrate to Digital Ocean Managed PostgreSQL for production.

#### Why Managed Database?
**Problems with Self-Hosted**:
- Data persistence concerns with Docker volumes
- Backup/restore complexity
- Resource management on single node
- No built-in high availability
- Manual maintenance (updates, patches, tuning)
- Disaster recovery difficult

**Benefits of Managed**:
- Automated daily backups (7-day retention)
- Point-in-time recovery
- Professional-grade reliability
- Automatic updates and patches
- Easy scaling (vertical and horizontal)
- Dedicated resources
- Built-in monitoring

#### Migration Process
1. **Created managed cluster** on Digital Ocean
2. **Updated Pulumi configs**:
   ```yaml
   postgres-host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com
   postgres-port: 25060
   ```
3. **Removed PostgreSQL service** from Layer 2
4. **Updated all service configurations** to use external database
5. **Migrated data** from old PostgreSQL to managed instance
6. **Updated `SprocketPostgresProvider`** to support external connections
   - Set `superuser=false` for managed DB

#### Results
- ‚úÖ PostgreSQL now an external dependency
- ‚úÖ Better reliability and performance
- ‚úÖ Automated backups
- ‚úÖ Reduced infrastructure complexity
- ‚ö†Ô∏è Added monthly cost (~$15-60/month)
- ‚ö†Ô∏è External dependency (requires internet connectivity)

**Lessons Learned**:
- Don't manage databases in production unless you have to
- Managed services are worth the cost for critical data
- Plan data migration carefully
- Update all connection configs atomically
- Document external dependencies

---

### Phase 7: The Final Push (Nov 4-8, 2025)
**Goal**: Deploy to production with real domain

#### Last Mile Challenges
- DNS propagation delays
- Let's Encrypt rate limiting
- Certificate provisioning timing
- Service health verification
- Final configuration tweaks

#### The Sprint
- **Nov 4** (`acf4e4e`): "so close!" - final debugging
- **Nov 7** (`f0f2ef5`): "Final stretch." - last mile work
- **Nov 7** (`e0d8785`): "Platform is up and running" - services deployed
- **Nov 8** (`a2862ea`): "Sprocket v1 is finally up and running completely." - **PRODUCTION COMPLETE!** üéâ

#### Final Configuration
```yaml
# Layer 1
hostname: sprocket.mlesports.gg
postgres-host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com
postgres-port: "25060"
vault-s3-endpoint: https://nyc3.digitaloceanspaces.com
vault-s3-bucket: vault-secrets

# Layer 2
hostname: sprocket.mlesports.gg
s3-endpoint: nyc3.digitaloceanspaces.com

# Platform
hostname: sprocket.mlesports.gg
subdomain: "main"
image-tag: main
postgres-host: sprocketbot-postgres-d5033d2-do-user-24528890-0.j.db.ondigitalocean.com
```

#### Verification
```bash
# All services running
docker service ls
# 15/15 services at 1/1 replicas

# HTTPS working
curl -I https://sprocket.mlesports.gg
# HTTP/2 200

# Certificates valid
openssl s_client -connect sprocket.mlesports.gg:443 -servername sprocket.mlesports.gg
# Issuer: Let's Encrypt
```

---

## Technical Challenges Deep Dive

### Challenge 1: Vault Bootstrapping Automation

**Complexity**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Highest)

**The Chicken-and-Egg Problem**:
```
Vault stores secrets ‚Üí Services need secrets ‚Üí Vault must be unsealed ‚Üí Unsealing needs keys ‚Üí Where to store keys?
```

**Attempts**:
1. Manual unsealing (not scalable)
2. Keys in Pulumi config (security concern)
3. Keys in MinIO (circular dependency)
4. Cloud KMS auto-unseal (adds complexity and cost)

**Final Solution**: Local bind mount with auto-initialization script

**Code**: `global/services/vault/scripts/auto-initialize.sh`
```bash
#!/bin/sh
set -e

vault server -config=/vault.hcl &
VAULT_PID=$!
sleep 5

export VAULT_ADDR="http://127.0.0.1:8200"

# Check if initialized
if ! vault status | grep -q "Initialized.*true"; then
  # Initialize and save tokens
  vault operator init -key-shares=5 -key-threshold=3 > /vault/unseal-tokens/unseal_tokens.txt
  echo "Vault initialized"
fi

# Unseal
if [ -f "/vault/unseal-tokens/unseal_tokens.txt" ]; then
  grep 'Unseal Key' /vault/unseal-tokens/unseal_tokens.txt | \
    awk '{print $4}' | head -3 | \
    while read key; do
      vault operator unseal $key
    done
  echo "Vault unsealed"
fi

# Keep container running
wait $VAULT_PID
```

**Why It Works**:
- S3 backend persists Vault state
- Unseal keys stored in bind-mounted directory on node
- Auto-initialization script handles both first-run and restarts
- Repeatable across deployments

**Trade-offs**:
- ‚úÖ Fully automated
- ‚úÖ Repeatable
- ‚úÖ No manual intervention
- ‚ö†Ô∏è Unseal keys on node filesystem (acceptable for single-node deployment)
- ‚ö†Ô∏è Not true auto-unseal (would need cloud KMS)

---

### Challenge 2: Multi-Environment Routing

**Complexity**: ‚≠ê‚≠ê‚≠ê‚≠ê

**The Problem**: Support 4 different access patterns with one codebase

| Access Pattern | Host Header | Use Case |
|----------------|-------------|----------|
| localhost | `sprocket.localhost` | Local development |
| LAN IP | `192.168.4.39` | Remote browser on same network |
| Tailscale IP | `100.110.185.84` | Remote access via VPN |
| Production domain | `sprocket.mlesports.gg` | Public internet |

**Why It's Hard**:
- Traefik routing is based on `Host` header
- Let's Encrypt only works with public domains
- Certificate handling different for each pattern
- Can't use multiple routers with same name

**Solution**: Conditional routing rules based on configuration

```typescript
// Build routing rule based on config
const hostRules = [];

if (hostname) {
  hostRules.push(`Host(\`${subdomain}.${hostname}\`)`);
}

if (serverIp) {
  hostRules.push(`Host(\`${serverIp}\`)`);
}

if (tailscaleIp) {
  hostRules.push(`Host(\`${tailscaleIp}\`)`);
}

const routingRule = hostRules.join(' || ');

// Apply to Traefik labels
new TraefikLabels(`sprocket-web-${env}`)
  .rule(routingRule)
  .targetPort(3000)
  .tls("lets-encrypt-tls")
```

**Configuration Strategy**:
```yaml
# Local development
hostname: localhost
server-ip: 192.168.4.39
tailscale-ip: 100.110.185.84

# Production
hostname: sprocket.mlesports.gg
# server-ip and tailscale-ip removed
```

**Results**:
- ‚úÖ Single codebase for all environments
- ‚úÖ Configuration-driven routing
- ‚úÖ Works for local dev and production
- ‚ö†Ô∏è IP-based routing is a workaround
- ‚ö†Ô∏è Complex routing rules

---

### Challenge 3: Secret Management Architecture

**Complexity**: ‚≠ê‚≠ê‚≠ê‚≠ê

**The Problem**: Secrets scattered across multiple systems

**Secret Sources**:
1. **Doppler**: OAuth credentials, API tokens, application secrets
2. **Vault**: Runtime secret distribution
3. **Pulumi Config**: Infrastructure secrets
4. **Docker Secrets**: Mounted into containers

**Solution**: Hierarchical secret management

```
Doppler (Source of Truth)
    ‚Üì
Bootstrap Script
    ‚Üì
Vault (Runtime Storage)
    ‚Üì
Docker Secrets
    ‚Üì
Application Services
```

**Implementation**: `scripts/bootstrap-vault-secrets.sh`
```bash
# Load from Doppler
export GOOGLE_CLIENT_ID=$(doppler secrets get GOOGLE_CLIENT_ID)
export GOOGLE_CLIENT_SECRET=$(doppler secrets get GOOGLE_CLIENT_SECRET)
# ... etc

# Provision to Vault
vault kv put platform/sprocket/manual/oauth/google \
  clientId="$GOOGLE_CLIENT_ID" \
  clientSecret="$GOOGLE_CLIENT_SECRET"
```

**Why This Works**:
- **Doppler**: Single source of truth, team access control
- **Vault**: Runtime distribution, dynamic secrets, auditing
- **Docker Secrets**: Secure container mounting
- **Pulumi**: Infrastructure-only secrets

**Secret Flow Example**:
```
Google OAuth credentials
  ‚Üí Stored in Doppler
  ‚Üí Bootstrap script reads from Doppler
  ‚Üí Writes to Vault at platform/sprocket/manual/oauth/google
  ‚Üí Pulumi reads from Vault
  ‚Üí Creates Docker secret
  ‚Üí Mounts into core service at /app/secret/googleClientId.txt
  ‚Üí Application reads from file
```

---

### Challenge 4: Storage Backend Migration

**Complexity**: ‚≠ê‚≠ê‚≠ê

**Why Migrate from MinIO?**
- Resource intensive (CPU, memory, storage)
- Another service to maintain
- Backup complexity
- Vault S3 backend instability with self-hosted MinIO

**Migration Strategy**: Phased approach

**Phase 1**: Dual operation
```typescript
// Support both local and cloud S3
export class SprocketMinioProvider {
  constructor(args: {
    vaultProvider?: vault.Provider,
    s3Endpoint?: string,
    accessKey?: string,
    secretKey?: string
  }) {
    if (args.vaultProvider) {
      // Use Vault for credentials
    } else {
      // Use direct credentials (for cloud S3)
    }
  }
}
```

**Phase 2**: Migrate Vault backend first
```hcl
# Vault config
storage "s3" {
  access_key = "${accessKey}"
  secret_key = "${secretKey}"
  bucket     = "vault-secrets"
  endpoint   = "https://nyc3.digitaloceanspaces.com"
  path       = "vault_storage"
}
```

**Phase 3**: Migrate application storage
- Updated service configurations
- Migrated existing data
- Verified all services working

**Phase 4**: Remove MinIO
- Removed service definition
- Cleaned up configuration
- Updated documentation

**Results**:
- ‚úÖ More reliable storage
- ‚úÖ Less infrastructure to manage
- ‚úÖ Better durability (99.999999999%)
- ‚úÖ Professional-grade backups

---

### Challenge 5: Let's Encrypt Certificate Management

**Complexity**: ‚≠ê‚≠ê‚≠ê

**The Problem**: Automatic TLS certificates for production

**Challenges**:
1. **Rate Limiting**: 50 certificates per week per domain
2. **DNS Requirements**: Domain must resolve to public IP
3. **Challenge Verification**: HTTP-01 challenge must reach server
4. **Local Development**: Can't get real certs for localhost

**Failure Modes Encountered**:
- Hit rate limit during testing (had to wait 1 hour)
- Tried to get certs before DNS propagated (failed)
- Mixed local/production configs (chaos)

**Solutions**:

**For Production**:
```yaml
# Traefik static config
certificatesResolvers:
  lets-encrypt-tls:
    acme:
      email: admin@sprocket.mlesports.gg
      storage: /data/acme.json
      httpChallenge:
        entryPoint: web
```

**For Local Development**:
```yaml
# No certificate resolver
# Use Traefik's default self-signed certs
# Accept certificate warnings in browser
```

**For Testing**:
```yaml
# Use Let's Encrypt staging
certificatesResolvers:
  lets-encrypt-tls:
    acme:
      caServer: https://acme-staging-v02.api.letsencrypt.org/directory
```

**Best Practices**:
1. Always test with staging first
2. Verify DNS before requesting prod certs
3. Persistent storage for `acme.json` (don't lose certs!)
4. One cert provisioning attempt takes 60+ seconds
5. Rate limits are per domain

---

## Architecture Evolution

### Before (2024)
```
Single-layer deployment
  ‚îú‚îÄ All services in one stack
  ‚îú‚îÄ Self-hosted PostgreSQL
  ‚îú‚îÄ Self-hosted MinIO
  ‚îú‚îÄ Manual Vault unsealing
  ‚îú‚îÄ No automated secret provisioning
  ‚îî‚îÄ Inconsistent configuration
```

### After (2025)
```
Three-layer architecture
  ‚îú‚îÄ Layer 1 (Infrastructure)
  ‚îÇ   ‚îú‚îÄ Traefik (reverse proxy)
  ‚îÇ   ‚îú‚îÄ Vault (secrets, S3 backend)
  ‚îÇ   ‚îî‚îÄ Socket Proxy
  ‚îÇ
  ‚îú‚îÄ Layer 2 (Data Services)
  ‚îÇ   ‚îú‚îÄ Redis
  ‚îÇ   ‚îú‚îÄ RabbitMQ
  ‚îÇ   ‚îú‚îÄ InfluxDB
  ‚îÇ   ‚îú‚îÄ Grafana
  ‚îÇ   ‚îú‚îÄ N8n
  ‚îÇ   ‚îú‚îÄ Neo4j
  ‚îÇ   ‚îú‚îÄ Gatus
  ‚îÇ   ‚îú‚îÄ Loki
  ‚îÇ   ‚îî‚îÄ Telegraf
  ‚îÇ
  ‚îî‚îÄ Platform (Applications)
      ‚îú‚îÄ Sprocket Web
      ‚îú‚îÄ Sprocket API
      ‚îú‚îÄ Discord Bot
      ‚îú‚îÄ Image Generation (frontend & service)
      ‚îî‚îÄ 6 microservices
```

### External Services
```
Critical Dependencies
  ‚îú‚îÄ Digital Ocean Managed PostgreSQL
  ‚îú‚îÄ Digital Ocean Spaces (Vault backend)
  ‚îú‚îÄ AWS S3 (application storage)
  ‚îú‚îÄ Doppler (secret management)
  ‚îú‚îÄ GitHub Organization (Vault auth)
  ‚îî‚îÄ DNS Provider (Let's Encrypt)
```

---

## Key Decisions and Rationale

### Decision 1: Three-Layer Architecture

**Rationale**:
- Clear separation of concerns
- Deploy layers independently
- Each layer depends on previous layers
- Easier troubleshooting
- Better resource isolation

**Trade-offs**:
- ‚úÖ More organized
- ‚úÖ Easier to understand
- ‚úÖ Better deployment control
- ‚ö†Ô∏è More complex deployment process
- ‚ö†Ô∏è Layer dependencies must be managed

---

### Decision 2: Managed PostgreSQL

**Rationale**:
- Critical data requires professional-grade reliability
- Automated backups essential
- Point-in-time recovery needed
- Team doesn't want to manage DB operations
- Single node deployment limits HA options

**Trade-offs**:
- ‚úÖ Better reliability
- ‚úÖ Automated backups
- ‚úÖ Less operational burden
- ‚úÖ Professional support
- ‚ö†Ô∏è Monthly cost ($15-60)
- ‚ö†Ô∏è External dependency
- ‚ö†Ô∏è Less control over tuning

**ROI**: Worth it for production. The time saved on maintenance and peace of mind from automated backups easily justifies the cost.

---

### Decision 3: Cloud S3 Storage

**Rationale**:
- S3-compatible storage needed for Vault backend
- MinIO adds operational complexity
- Cloud storage more reliable
- Better durability guarantees

**Trade-offs**:
- ‚úÖ More reliable
- ‚úÖ Less to manage
- ‚úÖ Better durability
- ‚úÖ Professional-grade
- ‚ö†Ô∏è Monthly cost (minimal)
- ‚ö†Ô∏è External dependency

**ROI**: Absolutely worth it. MinIO was causing more problems than it solved.

---

### Decision 4: Doppler for Secret Management

**Rationale**:
- Need centralized secret management
- Team collaboration on secrets
- Audit trail for secret access
- Better than storing secrets in Pulumi config

**Trade-offs**:
- ‚úÖ Centralized management
- ‚úÖ Team access control
- ‚úÖ Audit trail
- ‚úÖ Easy updates
- ‚ö†Ô∏è External dependency
- ‚ö†Ô∏è Potential vendor lock-in

**Alternative Considered**: AWS Secrets Manager (didn't want to lock into AWS)

---

### Decision 5: Local Bind Mount for Vault Unseal Keys

**Rationale**:
- Need automated unsealing
- Cloud KMS too complex and costly for single-node
- Pulumi config doesn't feel right for unseal keys
- Acceptable risk for single-node deployment

**Trade-offs**:
- ‚úÖ Fully automated
- ‚úÖ Simple implementation
- ‚úÖ Repeatable
- ‚ö†Ô∏è Keys on node filesystem
- ‚ö†Ô∏è Not true auto-unseal

**Future Consideration**: Migrate to cloud KMS auto-unseal if multi-node deployment needed.

---

## Metrics and Statistics

### Project Metrics
- **Duration**: 8 weeks (Sept 14 - Nov 8, 2025)
- **Total Commits**: 20+ commits in rebuild phase
- **Major Breakthroughs**: 5 key milestones
- **Services Deployed**: 22 total services
  - 3 in Layer 1
  - 9 in Layer 2
  - 10 in Platform
- **External Dependencies**: 6 critical systems

### Deployment Complexity
- **Pulumi Stacks**: 3 (layer_1, layer_2, prod)
- **Docker Networks**: 5 overlay networks
- **Docker Volumes**: ~15 persistent volumes
- **Configuration Files**: 50+ JSON/YAML configs
- **Secret Paths**: 20+ Vault paths
- **Infrastructure as Code**: ~5,000 lines of TypeScript

### Git Activity
- **Code Additions**: Significant refactoring
- **Code Deletions**: Removed deprecated services
- **Documentation**: Added 4 major docs (README, CLOUD_DEPLOYMENT, LOCAL_ACCESS, this postmortem)

---

## What Went Well

### 1. Iterative Approach
- Small changes, frequent commits
- Test each layer independently
- Rollback easy if something breaks

### 2. Comprehensive Documentation
- Documented decisions as they were made
- Detailed commit messages
- Created guides for different scenarios (local, cloud)

### 3. Migration to Managed Services
- PostgreSQL migration smooth
- Storage migration phased well
- Reduced operational complexity significantly

### 4. Automation
- Vault unsealing automated
- Secret provisioning scripted
- Deployment repeatable

### 5. Problem-Solving Persistence
- Vault unsealing took 5+ attempts
- Routing issues resolved through iteration
- Didn't give up when things got hard

---

## What Could Have Been Better

### 1. Database from Day One
**Issue**: Started with self-hosted PostgreSQL, then migrated to managed
**Better Approach**: Start with managed PostgreSQL from the beginning
**Impact**: Would have saved time and complexity

### 2. Storage Strategy Earlier
**Issue**: Spent time setting up MinIO, then migrated away
**Better Approach**: Use cloud S3 from the start
**Impact**: Would have avoided migration effort

### 3. Earlier Documentation
**Issue**: Some decisions documented after the fact
**Better Approach**: Document architectural decisions as they're made (ADRs)
**Impact**: Better context for future maintainers

### 4. Testing Environments
**Issue**: Tested some changes directly in production config
**Better Approach**: Use Let's Encrypt staging, have dev/staging/prod stacks
**Impact**: Would have avoided rate limiting

### 5. Initial Planning
**Issue**: Underestimated complexity, especially Vault automation
**Better Approach**: Research Vault best practices before starting
**Impact**: Might have gone with cloud KMS auto-unseal from start

---

## Lessons Learned

### Technical Lessons

#### 1. Use Managed Services for Critical Infrastructure
**Lesson**: Don't self-host databases or storage in production unless you have a really good reason.

**Rationale**:
- Managed services are worth the cost
- Time saved on maintenance is valuable
- Professional-grade reliability and backups
- Team can focus on application, not infrastructure

**Application**: Migrated both PostgreSQL and S3 to managed services. Both decisions were correct.

---

#### 2. Automate or Document, Never "Just Know"
**Lesson**: If it requires human memory or manual steps, automate it or document it thoroughly.

**Examples**:
- ‚úÖ Vault unsealing: Automated with script
- ‚úÖ Secret provisioning: Scripted bootstrap process
- ‚úÖ Deployment order: Documented in README
- ‚ùå Some manual DNS changes: Should be in Terraform/Pulumi

**Application**: Created scripts for repeatable processes, documentation for manual steps.

---

#### 3. Design for Production First
**Lesson**: Design infrastructure for production, then add development overrides.

**Anti-pattern**:
```yaml
# Starting with localhost, then trying to make it work in production
hostname: localhost
# Now how do I deploy to production?
```

**Better Pattern**:
```yaml
# Production-first design
hostname: sprocket.mlesports.gg

# Local development overrides
# hostname: localhost
# server-ip: 192.168.4.39
```

**Application**: Routing system designed for production domains, with optional IP-based access for development.

---

#### 4. Secrets Management Requires Strategy
**Lesson**: Secret management is more than just "keep them safe" - you need a coherent strategy.

**Our Strategy**:
1. **Source of Truth**: Doppler (team access, audit trail)
2. **Runtime Distribution**: Vault (dynamic secrets, policies)
3. **Container Secrets**: Docker secrets (secure mounting)
4. **Infrastructure Secrets**: Pulumi config (encrypted)

**Why It Works**:
- Clear ownership of each secret type
- Team can manage application secrets via Doppler
- Infrastructure secrets separated
- Auditable access

---

#### 5. Test the Full Flow, Not Just Individual Components
**Lesson**: Integration testing is critical for infrastructure.

**Example**: Vault unsealing
- ‚úÖ Vault initializes correctly
- ‚úÖ Unseal script works
- ‚úÖ Secrets can be written
- ‚ùå But does the *entire flow* work on container restart?

**Application**: Created verification scripts that test end-to-end flows:
- `quick-test.sh`: Basic health checks
- `verify-deployment.sh`: Comprehensive verification

---

#### 6. Git History Is Invaluable
**Lesson**: Detailed commit messages are worth the effort.

**Good Commit**:
```
feat(layer_2): migrate to cloud S3 and fix service configurations

Major changes:
- Replaced local MinIO service with cloud S3-compatible storage
- Updated SprocketMinioProvider to support direct credentials
- Fixed N8n PostgreSQL port configuration
- Removed postgres duplicate parameter

Breaking changes:
- Services now require s3-endpoint, s3-access-key, s3-secret-key config
```

**Bad Commit**:
```
updates
```

**Application**: This postmortem was possible because of detailed commit messages. Future debugging will be easier.

---

#### 7. External Dependencies Must Be Documented
**Lesson**: Any external system your infrastructure depends on must be clearly documented.

**Our External Dependencies**:
1. Doppler (secrets)
2. GitHub Organization (Vault auth)
3. Digital Ocean (database, storage)
4. DNS Provider (Let's Encrypt)
5. Pulumi Backend (state)
6. Docker Hub (private images)

**Application**: Created `EXTERNALITIES_AND_DEPENDENCIES.md` with:
- What each dependency provides
- Why it's required
- What happens if it's unavailable
- How to set it up

---

### Process Lessons

#### 1. Break Big Problems Into Smaller Pieces
**Lesson**: Don't try to solve everything at once.

**Example**: Vault automation
1. First: Get Vault running
2. Then: Make initialization work
3. Then: Make unsealing work
4. Then: Make it repeatable
5. Finally: Make it automated

**Application**: Layered architecture itself is an example - Layer 1 ‚Üí Layer 2 ‚Üí Platform.

---

#### 2. Document As You Learn
**Lesson**: Write documentation while the context is fresh.

**Examples**:
- Sept 19: "docs: update readme with learnings" (while debugging Vault)
- Oct 27: Created CLOUD_DEPLOYMENT.md (right after solving routing)
- Nov 8: Created comprehensive documentation package

**Application**: If we'd waited until the end, we'd have forgotten crucial details.

---

#### 3. Persistence Pays Off
**Lesson**: Complex infrastructure problems often require multiple attempts.

**Example**: Vault unsealing
- Attempt 1: Failed (keys in Pulumi)
- Attempt 2: Failed (manual unsealing)
- Attempt 3: Failed (keys in MinIO)
- Attempt 4: Failed (initialization issues)
- Attempt 5: Success! (bind mount + auto-initialize script)

**Application**: Don't give up on the first failure. Learn from each attempt.

---

#### 4. Plan for Multiple Environments
**Lesson**: Local development and production are different beasts.

**Differences**:
| Aspect | Local | Production |
|--------|-------|------------|
| DNS | `.localhost` | Real domain |
| Certificates | Self-signed | Let's Encrypt |
| Routing | IP-based | Host-based |
| Access | Local network | Public internet |

**Application**: Created separate documentation for each environment (LOCAL_ACCESS.md, CLOUD_DEPLOYMENT.md).

---

## Recommendations for Future Projects

### 1. Start with Managed Services
For any new infrastructure project:
- ‚úÖ Use managed databases (RDS, Digital Ocean, etc.)
- ‚úÖ Use managed storage (S3, Spaces, GCS)
- ‚úÖ Use managed secrets (Doppler, AWS Secrets Manager)
- ‚ùå Don't self-host unless you have a specific reason

**Rationale**: Your time is valuable. Let professionals manage infrastructure.

---

### 2. Invest in Automation Early
Don't wait until deployment is painful to automate:
- Write deployment scripts from day one
- Automate secret provisioning
- Create verification scripts
- Document the deployment process

**Rationale**: Manual deployments don't scale and lead to errors.

---

### 3. Use Infrastructure as Code
Everything should be in code:
- ‚úÖ Infrastructure (Pulumi, Terraform)
- ‚úÖ Configuration (YAML, JSON in git)
- ‚úÖ Secrets (references, not values)
- ‚úÖ Deployment scripts (bash, make)

**Rationale**: If it's not in code, it's not repeatable.

---

### 4. Document Architectural Decisions
Create ADRs (Architectural Decision Records) for major decisions:
- Why did we choose X over Y?
- What were the trade-offs?
- What assumptions did we make?
- When should we revisit this decision?

**Rationale**: Future you (and your team) will thank you.

---

### 5. Plan for Disaster Recovery
Before going to production, answer these questions:
- What if the database goes down?
- What if Vault is lost?
- What if Pulumi state is corrupted?
- What if we lose access to Doppler?
- How do we restore from backups?

**Rationale**: Hope for the best, plan for the worst.

---

## Open Questions and Future Work

### 1. Multi-Node Deployment
**Question**: How would this architecture scale to multiple nodes?

**Considerations**:
- Vault unsealing (cloud KMS auto-unseal?)
- Traefik load balancing
- Persistent volume management
- Database connection pooling

**Recommendation**: Test with 3-node setup before scaling to production.

---

### 2. Disaster Recovery
**Question**: Can we fully restore from scratch?

**Gaps**:
- Vault unseal keys (currently on node filesystem)
- Pulumi state (if backend is lost)
- Doppler secrets (if account compromised)

**Recommendation**: Document and test full disaster recovery procedure.

---

### 3. Cost Optimization
**Question**: Can we reduce monthly costs?

**Current Costs**:
- Digital Ocean Managed PostgreSQL: $15-60/month
- Digital Ocean Spaces: $5/month
- AWS S3: Variable
- Doppler: Free tier (for now)
- Pulumi: Free tier (for now)

**Recommendation**: Review costs quarterly, look for optimization opportunities.

---

### 4. Monitoring and Alerting
**Question**: How do we know when things break?

**Current State**:
- ‚úÖ Gatus for service monitoring
- ‚úÖ Grafana for metrics
- ‚úÖ Loki for logs
- ‚ùå No alerting configured
- ‚ùå No on-call rotation

**Recommendation**: Set up PagerDuty or similar, configure alerts.

---

### 5. Security Hardening
**Question**: Is the infrastructure secure enough for production?

**Current State**:
- ‚úÖ Secrets in Vault, not in code
- ‚úÖ TLS everywhere
- ‚úÖ Minimal exposed ports
- ‚ùå No regular security audits
- ‚ùå No automated vulnerability scanning
- ‚ùå No WAF or DDoS protection

**Recommendation**: Security audit, implement automated scanning, consider Cloudflare.

---

## Conclusion

The Sprocket infrastructure rebuild was a complex, 8-week journey from a partially-functional system to a production-ready platform. The project involved fundamental architectural changes, migration to managed services, and resolution of challenging technical problems.

### Key Achievements
- ‚úÖ **Production-ready infrastructure** serving real users
- ‚úÖ **Three-layer architecture** with clear separation of concerns
- ‚úÖ **Managed services** for critical components (database, storage)
- ‚úÖ **Automated processes** for deployment and secret management
- ‚úÖ **Comprehensive documentation** for future maintenance

### Critical Success Factors
1. **Persistence**: Many problems required multiple attempts
2. **Iteration**: Small changes, frequent commits, test often
3. **Managed Services**: Database and storage migrations simplified operations
4. **Documentation**: Writing things down as we learned saved the project
5. **Team Collaboration**: Doppler and GitHub org enabled team access

### Most Important Lessons
1. Use managed services for critical infrastructure
2. Design for production first, add dev overrides later
3. Automate secret provisioning to reduce human error
4. Document decisions when making them, not after
5. Test each layer independently before stacking
6. Git history is invaluable for understanding evolution

### Final Thoughts

This project demonstrates that even complex infrastructure challenges can be solved through:
- Systematic problem-solving
- Willingness to iterate and learn
- Good documentation practices
- Appropriate use of managed services
- Team collaboration

The result is a robust, maintainable infrastructure that serves as a foundation for the Sprocket gaming platform. While there's always room for improvement (security hardening, better monitoring, disaster recovery testing), the infrastructure is production-ready and serving users successfully.

**The most valuable output of this project isn't just working infrastructure - it's the knowledge captured in documentation, code, and this postmortem that will enable future teams to maintain, extend, and learn from this work.**

---

**Project Status**: ‚úÖ COMPLETE
**Documentation Status**: ‚úÖ COMPREHENSIVE
**Production Status**: ‚úÖ LIVE AND SERVING USERS
**Team Satisfaction**: üéâ HIGH

---

*This postmortem was generated from project documentation, git history, and architectural analysis. For detailed technical information, see the companion documents: TECHNICAL_CHALLENGES.md, CONTEXT_SUMMARY.md, EXTERNALITIES_AND_DEPENDENCIES.md, and GIT_HISTORY_ANALYSIS.md.*
